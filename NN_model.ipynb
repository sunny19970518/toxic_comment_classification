{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "# from subprocess import check_output\n",
    "# print(check_output([\"ls\", \"data\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Bidirectional, GlobalMaxPool1D\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers import GRU, LSTM\n",
    "from tensorflow.keras.models import model\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessing!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\l8527\\.conda\\envs\\tf-image\\lib\\site-packages\\pandas\\core\\series.py:4479: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().replace(\n",
      "<ipython-input-1-7e940b979d7f>:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['Processed_msg']=clean_msg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessing Ends!!!\n",
      "Data Preprocessing!!!\n",
      "Data Preprocessing Ends!!!\n",
      "Epoch 1/5\n",
      "160/160 [==============================] - 19s 101ms/step - loss: 0.2736 - accuracy: 0.3943\n",
      "Epoch 2/5\n",
      "160/160 [==============================] - 16s 102ms/step - loss: 0.1250 - accuracy: 0.7829\n",
      "Epoch 3/5\n",
      "160/160 [==============================] - 16s 103ms/step - loss: 0.1042 - accuracy: 0.9170\n",
      "Epoch 4/5\n",
      "160/160 [==============================] - 16s 102ms/step - loss: 0.0822 - accuracy: 0.9730\n",
      "Epoch 5/5\n",
      "160/160 [==============================] - 16s 102ms/step - loss: 0.0771 - accuracy: 0.9749\n"
     ]
    }
   ],
   "source": [
    "def lemmatize_all(sentence):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    for word, tag in pos_tag(word_tokenize(sentence)):\n",
    "        if tag.startswith(\"NN\"):\n",
    "            yield wnl.lemmatize(word, pos='n')\n",
    "        elif tag.startswith('VB'):\n",
    "            yield wnl.lemmatize(word, pos='v')\n",
    "        elif tag.startswith('JJ'):\n",
    "            yield wnl.lemmatize(word, pos='a')\n",
    "        elif tag.startswith('R'):\n",
    "            yield wnl.lemmatize(word, pos='r')\n",
    "            \n",
    "        else:\n",
    "            yield word\n",
    "\n",
    "\n",
    "def msgProcessing(raw_msg):\n",
    "    m_w=[]\n",
    "    words2=[]\n",
    "    raw_msg=str(raw_msg)\n",
    "    raw_msg = str(raw_msg.lower())\n",
    "    #url_stripper= re.sub(r'Email me.*[A-Z]',\"\",s)\n",
    "    \n",
    "    #raw_msg=re.sub(r'\\w*[0-9]\\w*','', url_stripper)\n",
    "    raw_msg=re.sub(r'[^a-zA-Z]', ' ', raw_msg)\n",
    "    \n",
    "    words=raw_msg.lower().split()\n",
    "    #Remove words with length lesser than 3 if not w in stops\n",
    "    for i in words:\n",
    "        if len(i)>=2:\n",
    "            words2.append(i)\n",
    "    stops=set(stopwords.words('english'))\n",
    "    m_w=\" \".join([w for w in words2])\n",
    "    return(\" \".join(lemmatize_all(m_w)))\n",
    "\n",
    "\n",
    "def helperFunction(df):\n",
    "    print (\"Data Preprocessing!!!\")\n",
    "    cols=['comment_text']\n",
    "    df=df[cols]\n",
    "    df.comment_text.replace({r'[^\\x00-\\x7F]+':''},regex=True,inplace=True)\n",
    "    num_msg=df[cols].size\n",
    "    clean_msg=[]\n",
    "    for i in range(0,num_msg):\n",
    "        clean_msg.append(msgProcessing(df['comment_text'][i]))\n",
    "    df['Processed_msg']=clean_msg\n",
    "    X=df['Processed_msg']\n",
    "    print (\"Data Preprocessing Ends!!!\")\n",
    "    return X\n",
    "\n",
    "\n",
    "def embedding(train,test):\n",
    "    tokenizer = Tokenizer(num_words=20000)\n",
    "    tokenizer.fit_on_texts(train)\n",
    "    t=len(tokenizer.word_index)+1\n",
    "    trainsequences = tokenizer.texts_to_sequences(train)\n",
    "    traindata = pad_sequences(trainsequences, maxlen=100)\n",
    "    testsequences = tokenizer.texts_to_sequences(test)\n",
    "    testdata = pad_sequences(testsequences, maxlen=100)\n",
    "    return traindata, testdata,t\n",
    "\n",
    "\n",
    "\n",
    "def getTarget(y):\n",
    "    ytrain=y[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "    return ytrain\n",
    "\n",
    "\n",
    "\n",
    "df= pd.read_csv(\"data/train.csv\",encoding='latin-1')\n",
    "\n",
    "\n",
    "\n",
    "X=helperFunction(df)\n",
    "\n",
    "\n",
    "\n",
    "df2=pd.read_csv(\"data/test.csv\",encoding='latin-1')\n",
    "df2['comment_text'].fillna('Missing',inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "X2=helperFunction(df2)\n",
    "\n",
    "\n",
    "\n",
    "xtrain,xtest,vocab_size=embedding(X,X2)\n",
    "\n",
    "\n",
    "\n",
    "classes=[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "ytrain=getTarget(df[classes])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_CNN_GRU_Model(xtrain,ytrain):\n",
    "    batch_size=1000\n",
    "    epochs=5\n",
    "    model= Sequential()\n",
    "    model.add(Embedding(20000,32,input_length=100))\n",
    "    model.add(Conv1D(32,kernel_size=3,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv1D(64,kernel_size=3,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(Conv1D(128,kernel_size=3,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(GRU(50,return_sequences=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dropout(0.45))\n",
    "    model.add(Dense(6,activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    model.fit(xtrain,ytrain,batch_size=batch_size,epochs=epochs)\n",
    "    model.save(\"toxic_CNN_GRU.h5\")\n",
    "    pred=model.predict(xtest)\n",
    "    return pred\n",
    "\n",
    "\n",
    "\n",
    "pred= build_CNN_GRU_Model(xtrain,ytrain)\n",
    "\n",
    "\n",
    "\n",
    "def saveCSV(ytest):\n",
    "    sample_submission = pd.read_csv(\"data/sample_submission.csv\",encoding='latin-1')\n",
    "    sample_submission[classes] = ytest\n",
    "    sample_submission.to_csv(\"toxic.csv\", index=False)\n",
    "\n",
    "\n",
    "saveCSV(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "160/160 [==============================] - 16s 95ms/step - loss: 0.2508 - accuracy: 0.4101\n",
      "Epoch 2/5\n",
      "160/160 [==============================] - 15s 96ms/step - loss: 0.1212 - accuracy: 0.7579\n",
      "Epoch 3/5\n",
      "160/160 [==============================] - 15s 96ms/step - loss: 0.1120 - accuracy: 0.8825\n",
      "Epoch 4/5\n",
      "160/160 [==============================] - 15s 97ms/step - loss: 0.1024 - accuracy: 0.9577\n",
      "Epoch 5/5\n",
      "160/160 [==============================] - 16s 102ms/step - loss: 0.0840 - accuracy: 0.9860\n"
     ]
    }
   ],
   "source": [
    "def build_CNN_Model(xtrain,ytrain):\n",
    "    batch_size=1000\n",
    "    epochs=5\n",
    "    model= Sequential()\n",
    "    model.add(Embedding(20000,32,input_length=100))\n",
    "    model.add(Conv1D(32,kernel_size=3,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv1D(64,kernel_size=3,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(Conv1D(128,kernel_size=3,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dropout(0.45))\n",
    "    model.add(Dense(6,activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    model.fit(xtrain,ytrain,batch_size=batch_size,epochs=epochs)\n",
    "    model.save(\"toxic_CNN.h5\")\n",
    "    pred=model.predict(xtest)\n",
    "    return pred\n",
    "\n",
    "pred= build_CNN_Model(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "160/160 [==============================] - 21s 114ms/step - loss: 0.2834 - accuracy: 0.4441\n",
      "Epoch 2/5\n",
      "160/160 [==============================] - 17s 106ms/step - loss: 0.1288 - accuracy: 0.7651\n",
      "Epoch 3/5\n",
      "160/160 [==============================] - 17s 105ms/step - loss: 0.1144 - accuracy: 0.9048\n",
      "Epoch 4/5\n",
      "160/160 [==============================] - 17s 105ms/step - loss: 0.1051 - accuracy: 0.9625\n",
      "Epoch 5/5\n",
      "160/160 [==============================] - 17s 107ms/step - loss: 0.0922 - accuracy: 0.9798\n"
     ]
    }
   ],
   "source": [
    "def build_CNN_LSTM_Model(xtrain,ytrain):\n",
    "    batch_size=1000\n",
    "    epochs=5\n",
    "    model= Sequential()\n",
    "    model.add(Embedding(20000,32,input_length=100))\n",
    "    model.add(Conv1D(32,kernel_size=3,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv1D(64,kernel_size=3,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Dropout(0.35))\n",
    "    model.add(Conv1D(128,kernel_size=3,padding='same',activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=3))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(LSTM(50,return_sequences=True))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dropout(0.45))\n",
    "    model.add(Dense(6,activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    model.fit(xtrain,ytrain,batch_size=batch_size,epochs=epochs)\n",
    "    model.save(\"toxic_CNN_LSTM.h5\")\n",
    "    pred=model.predict(xtest)\n",
    "    return pred\n",
    "\n",
    "pred= build_CNN_LSTM_Model(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "160/160 [==============================] - 38s 225ms/step - loss: 0.3434 - accuracy: 0.4984\n",
      "Epoch 2/5\n",
      "160/160 [==============================] - 37s 233ms/step - loss: 0.1005 - accuracy: 0.8201\n",
      "Epoch 3/5\n",
      "160/160 [==============================] - 38s 235ms/step - loss: 0.0573 - accuracy: 0.9431\n",
      "Epoch 4/5\n",
      "160/160 [==============================] - 37s 234ms/step - loss: 0.0508 - accuracy: 0.9809\n",
      "Epoch 5/5\n",
      "160/160 [==============================] - 37s 234ms/step - loss: 0.0473 - accuracy: 0.9899\n"
     ]
    }
   ],
   "source": [
    "def build_LSTM_Model(xtrain,ytrain):\n",
    "    batch_size=1000\n",
    "    epochs=5\n",
    "    model= Sequential()\n",
    "    model.add(Embedding(20000,32,input_length=100))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dropout(0.45))\n",
    "    model.add(Dense(6,activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    model.fit(xtrain,ytrain,batch_size=batch_size,epochs=epochs)\n",
    "    model.save(\"toxic_LSTM.h5\")\n",
    "    pred=model.predict(xtest)\n",
    "    return pred\n",
    "pred= build_LSTM_Model(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "160/160 [==============================] - 32s 190ms/step - loss: 0.3499 - accuracy: 0.3014\n",
      "Epoch 2/5\n",
      "160/160 [==============================] - 31s 191ms/step - loss: 0.0692 - accuracy: 0.8654\n",
      "Epoch 3/5\n",
      "160/160 [==============================] - 31s 194ms/step - loss: 0.0543 - accuracy: 0.9405\n",
      "Epoch 4/5\n",
      "160/160 [==============================] - 31s 194ms/step - loss: 0.0503 - accuracy: 0.9596\n",
      "Epoch 5/5\n",
      "160/160 [==============================] - 31s 191ms/step - loss: 0.0481 - accuracy: 0.9721\n"
     ]
    }
   ],
   "source": [
    "def build_GRU_Model(xtrain,ytrain):\n",
    "    batch_size=1000\n",
    "    epochs=5\n",
    "    model= Sequential()\n",
    "    model.add(Embedding(20000,32,input_length=100))\n",
    "    model.add(GRU(50))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dropout(0.45))\n",
    "    model.add(Dense(6,activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    model.fit(xtrain,ytrain,batch_size=batch_size,epochs=epochs)\n",
    "    model.save(\"toxic_GRU.h5\")\n",
    "    pred=model.predict(xtest)\n",
    "    return pred\n",
    "pred= build_GRU_Model(xtrain,ytrain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
